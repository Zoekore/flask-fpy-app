{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60124c35-785f-4400-bb45-83ebf5054de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n",
      "Model or encoder for motivation not found\n",
      "Model or encoder for satisfaction not found\n",
      "Model or encoder for challenge not found\n",
      "Model or encoder for learning not found\n",
      "Model or encoder for social not found\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [19/Jun/2025 23:23:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Jun/2025 23:23:05] \"GET /model_status HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Jun/2025 23:23:06] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define paths\n",
    "MODEL_DIR = 'farm_practice_models'\n",
    "\n",
    "# Define emotional aspects\n",
    "emotional_aspects = {\n",
    "    'motivation': ['motivat', 'inspir', 'drive', 'enthusias', 'passion', 'eager', 'excit'],\n",
    "    'satisfaction': ['satisf', 'content', 'happy', 'enjoy', 'proud', 'fulfill', 'achiev'],\n",
    "    'challenge': ['challeng', 'difficult', 'hard', 'struggl', 'frustrat', 'stress', 'overwhelm'],\n",
    "    'learning': ['learn', 'knowledge', 'skill', 'understand', 'educat', 'growth', 'develop'],\n",
    "    'social': ['team', 'collaborat', 'friend', 'interact', 'communit', 'connect', 'relation']\n",
    "}\n",
    "\n",
    "# Load models, tokenizer, and encoders\n",
    "aspect_models = {}\n",
    "aspect_encoders = {}\n",
    "\n",
    "# Check if models directory exists\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    print(f\"Created model directory at {MODEL_DIR}. Please place your models here.\")\n",
    "else:\n",
    "    # Load tokenizer if it exists\n",
    "    tokenizer_path = os.path.join(MODEL_DIR, 'tokenizer.pkl')\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        with open(tokenizer_path, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        print(\"Tokenizer loaded successfully\")\n",
    "    else:\n",
    "        print(f\"Tokenizer not found at {tokenizer_path}\")\n",
    "    \n",
    "    # Load aspect models and encoders\n",
    "    for aspect in emotional_aspects.keys():\n",
    "        model_path = os.path.join(MODEL_DIR, f'{aspect}_model')\n",
    "        encoder_path = os.path.join(MODEL_DIR, f'{aspect}_encoder.pkl')\n",
    "        \n",
    "        if os.path.exists(model_path) and os.path.exists(encoder_path):\n",
    "            try:\n",
    "                aspect_models[aspect] = load_model(model_path)\n",
    "                with open(encoder_path, 'rb') as f:\n",
    "                    aspect_encoders[aspect] = pickle.load(f)\n",
    "                print(f\"Loaded model and encoder for {aspect}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {aspect} model: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Model or encoder for {aspect} not found\")\n",
    "\n",
    "# Text preprocessing functions\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(f'[{string.punctuation}]', ' ', text)  # remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Initialize NLTK components\n",
    "    try:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    except:\n",
    "        import nltk\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Analyze sentiment for new text\n",
    "def analyze_farm_experience(text):\n",
    "    results = {}\n",
    "    \n",
    "    # Clean and process text\n",
    "    cleaned = clean_text(text)\n",
    "    processed = preprocess_text(cleaned)\n",
    "    \n",
    "    # Extract overall sentiment using rule-based approach\n",
    "    positive_words = ['enjoy', 'great', 'good', 'positive', 'happy', 'satisfied', 'exciting',\n",
    "                      'love', 'wonderful', 'excellent', 'amazing', 'fantastic', 'terrific']\n",
    "    negative_words = ['difficult', 'hard', 'challenging', 'frustrat', 'stress', 'negative',\n",
    "                      'overwhelm', 'disappointment', 'dislike', 'hate', 'terrible', 'awful', 'poor']\n",
    "    \n",
    "    positive_count = sum(1 for word in positive_words if word in processed)\n",
    "    negative_count = sum(1 for word in negative_words if word in processed)\n",
    "    \n",
    "    if positive_count > negative_count:\n",
    "        results['overall_sentiment'] = 'positive'\n",
    "    elif negative_count > positive_count:\n",
    "        results['overall_sentiment'] = 'negative'\n",
    "    else:\n",
    "        results['overall_sentiment'] = 'neutral'\n",
    "    \n",
    "    # Set maximum sequence length for padding\n",
    "    max_seq_length = 100\n",
    "    aspects_present = {}\n",
    "    \n",
    "    for aspect, keywords in emotional_aspects.items():\n",
    "        # Check if aspect is mentioned in the processed text\n",
    "        is_present = any(re.search(r'\\b' + keyword + r'[a-z]*\\b', processed) for keyword in keywords)\n",
    "        aspects_present[aspect] = is_present\n",
    "        \n",
    "        if is_present:\n",
    "            # If the model, encoder, or tokenizer is missing, mark as not available\n",
    "            if aspect not in aspect_models or aspect not in aspect_encoders or 'tokenizer' not in globals():\n",
    "                results[aspect] = 'model not available'\n",
    "            else:\n",
    "                # Convert text to sequence and pad\n",
    "                sequence = tokenizer.texts_to_sequences([processed])\n",
    "                padded_sequence = pad_sequences(sequence, maxlen=max_seq_length, padding='post')\n",
    "                \n",
    "                # Make prediction and convert to class label\n",
    "                prediction = aspect_models[aspect].predict(padded_sequence)[0]\n",
    "                encoder = aspect_encoders[aspect]\n",
    "                if len(encoder.classes_) == 2:\n",
    "                    idx = 1 if prediction > 0.5 else 0\n",
    "                else:\n",
    "                    idx = np.argmax(prediction)\n",
    "                results[aspect] = encoder.classes_[idx]\n",
    "        else:\n",
    "            results[aspect] = 'not_mentioned'\n",
    "    \n",
    "    # Include aspect presence information in results\n",
    "    results['aspects_present'] = aspects_present\n",
    "    return results\n",
    "\n",
    "# Routes\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/analyze', methods=['POST'])\n",
    "def analyze():\n",
    "    if request.method == 'POST':\n",
    "        text = request.form['experience_text']\n",
    "        if not text:\n",
    "            return jsonify({'error': 'Please enter some text to analyze'})\n",
    "        \n",
    "        try:\n",
    "\n",
    "            results = analyze_farm_experience(text)\n",
    "            return jsonify(results)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Error analyzing text: {str(e)}'})\n",
    "\n",
    "@app.route('/model_status')\n",
    "def model_status():\n",
    "    status = {\n",
    "        'tokenizer_loaded': 'tokenizer' in globals(),\n",
    "        'models_loaded': {aspect: aspect in aspect_models for aspect in emotional_aspects.keys()}\n",
    "    }\n",
    "\n",
    "    return jsonify(status)\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073c0ea-a532-4c3e-b808-a3429fd5faa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe1665-4738-44e3-a365-d18df6a98f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
